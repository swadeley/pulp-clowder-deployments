apiVersion: template.openshift.io/v1
kind: Template
labels:
  app: content-sources-pulp
  template: content-sources-pulp-clowdapp
metadata:
  annotations:
    description: |-
      A quick way to test and try Pulp running in OpenShift clusters. For more information about using this template see ...

      WARNING: Any data stored will be lost upon pod destruction. Only use this template for testing.
    openshift.io/display-name: Pulp (Ephemeral)
    openshift.io/documentation-url: https://docs.pulpproject.org/pulpcore/
    openshift.io/long-description: This template defines resources needed
      to install Pulp. The database is stored in non-persistent storage,
      so this configuration should be used for experimental purposes only.
    openshift.io/support-url: https://github.com/pulp/pulp-operator/issues
    samples.operator.openshift.io/version: 4.10.12
    tags: quickstart,pulp
    template.openshift.io/bindable: "false"
    iconClass: icon-python
  labels:
    samples.operator.openshift.io/managed: "false"
  name: content-sources-pulp-clowdapp
objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: cs-config-sh-map
  data:
    config-sh-file: |
      #!/bin/bash
      export PATH=${PATH}:/tmp/bin

      kubectl get secret
      retVal=$?
      echo $retVal
      if [ $retval -ne 0 ]; then
         echo "Stopping here because we are running on the staging environment or we cannot retrieve secrets"
         sleep 9999999999
         exit 0
      fi

      mkdir /tmp/bin
      cd /tmp/bin
      curl -L -O https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64
      mv jq-linux64 jq
      chmod +x jq

      # https://docs.pulpproject.org/pulp_operator/configuring/storage/#configuring-pulp-operator-to-use-object-storage
      S3_ACCESS_KEY_ID=$(cat /cdapp/cdappconfig.json | jq -r '.objectStore.buckets[0].accessKey')
      S3_SECRET_ACCESS_KEY=$(cat /cdapp/cdappconfig.json | jq -r '.objectStore.buckets[0].secretKey')
      S3_BUCKET_NAME=$(cat /cdapp/cdappconfig.json | jq -r '.objectStore.buckets[0].name')
      #S3_REGION='us-east-1'
      S3_HOSTNAME=$(cat /cdapp/cdappconfig.json | jq -r '.objectStore.hostname')

      # FIXME: Find out why these 2 `kubectl apply` commands keep getting killed.
      # It is hard to debug inside the container. OOM could be to blame, but we cannot run `dmesg`.
      until kubectl apply -f- <<EOF
      apiVersion: v1
      kind: Secret
      metadata:
        name: 'content-sources-pulp-s3-for-operator'
      stringData:
        s3-access-key-id: $S3_ACCESS_KEY_ID
        s3-secret-access-key: $S3_SECRET_ACCESS_KEY
        s3-bucket-name: $S3_BUCKET_NAME
        s3-endpoint: http://${S3_HOSTNAME}:9000
        s3-region: east
      EOF
      do
        echo "Trying kubectl apply again"
        sleep 1
      done
      # s3-region: $S3_REGION
      
      # https://docs.pulpproject.org/pulp_operator/configuring/cache/#configuring-pulp-operator-to-use-an-external-redis-installation
      
      kubectl create secret generic content-sources-pulp-in-memory-db \
              --from-literal=REDIS_HOST=$(cat /cdapp/cdappconfig.json | jq -r '.inMemoryDb.hostname') \
              --from-literal=REDIS_PORT=$(cat /cdapp/cdappconfig.json | jq -r '.inMemoryDb.port') \
              --from-literal=REDIS_PASSWORD=""  \
              --from-literal=REDIS_DB=""
      
      # https://docs.pulpproject.org/pulp_operator/configuring/database/#configuring-pulp-operator-to-use-an-external-postgresql-installation
      
      kubectl create secret generic content-sources-pulp-external-database \
              --from-literal=POSTGRES_HOST=$(cat /cdapp/cdappconfig.json | jq -r '.database.hostname')  \
              --from-literal=POSTGRES_PORT=5432  \
              --from-literal=POSTGRES_USERNAME=$(cat /cdapp/cdappconfig.json | jq -r '.database.adminUsername')  \
              --from-literal=POSTGRES_PASSWORD=$(cat /cdapp/cdappconfig.json | jq -r '.database.adminPassword')  \
              --from-literal=POSTGRES_DB_NAME=$(cat /cdapp/cdappconfig.json | jq -r '.database.name') \
              --from-literal=POSTGRES_SSLMODE=disable
      
      echo "Dummy output so that 'bonfire deploy --set-image-tag' will not choke, it requires this string to be present"
      echo "${IMAGE}:${IMAGE_TAG}"

      # FIXME: Look into having the container/pod run and stop, probably a "job" rather than a "deployment" in the clowdapp.
      # For now, it is useful to have the container running. We can run commands on it.
      sleep 9999999999

- apiVersion: operators.coreos.com/v1alpha1
  kind: Subscription
  metadata:
    labels:
      operators.coreos.com/pulp-operator.pulp: ""
    name: pulp-operator
  # namespace: "${ENV_NAME}"
  spec:
    channel: beta
    installPlanApproval: Automatic
    name: pulp-operator
    source: community-operators
    sourceNamespace: openshift-marketplace
    startingCSV: pulp-operator.v1.0.0-alpha.9
    config:
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "128Mi"
          cpu: "250m"
- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: content-sources-pulp
  #  namespace: "${ENV_NAME}"
  spec:
    envName: "${ENV_NAME}"
    testing:
      iqePlugin: content-sources
    deployments:
    - name: app
      podSpec:
        # dummy container
        image: "quay.io/openshift/origin-cli:latest"
        command: ['bash', '-x', '/tmp/config.sh']
        volumes:
        - name: config-sh
          configMap:
            name: cs-config-sh-map
        volumeMounts:
        - name: config-sh
          mountPath: /tmp/config.sh
          subPath: config-sh-file
    database:
      name: content-sources-pulp
      version: 15
    inMemoryDb: true
    objectStore:
      - content-sources-pulp-s3
- apiVersion: repo-manager.pulpproject.org/v1beta2
  kind: Pulp
  metadata:
    name: cs-pulp
  spec:
    api:
      replicas: ${{PULP_API_REPLICAS}}
      gunicorn_workers: 1
      resource_requirements:
        requests:
          cpu: ${{PULP_API_CPU_REQUEST}}
          memory: ${{PULP_API_MEMORY_REQUEST}}
        limits:
          cpu: ${{PULP_API_CPU_LIMIT}}
          memory: ${{PULP_API_MEMORY_LIMIT}}
    content:
      replicas: ${{PULP_CONTENT_REPLICAS}}
      gunicorn_workers: 1
      resource_requirements:
        requests:
          cpu: ${{PULP_CONTENT_CPU_REQUEST}}
          memory: ${{PULP_CONTENT_MEMORY_REQUEST}}
        limits:
          cpu: ${{PULP_API_CPU_LIMIT}}
          memory: ${{PULP_CONTENT_MEMORY_LIMIT}}
    web:
      replicas: ${{PULP_WEB_REPLICAS}}
      resource_requirements:
        requests:
          cpu: ${{PULP_WEB_CPU_REQUEST}}
          memory: ${{PULP_WEB_MEMORY_REQUEST}}
        limits:
          cpu: ${{PULP_WEB_CPU_LIMIT}}
          memory: ${{PULP_WEB_MEMORY_LIMIT}}
    worker:
      replicas: ${{PULP_WORKER_REPLICAS}}
      resource_requirements:
        requests:
          cpu: ${{PULP_WORKER_CPU_REQUEST}}
          memory: ${{PULP_WORKER_MEMORY_REQUEST}}
        limits:
          cpu: ${{PULP_WORKER_CPU_LIMIT}}
          memory: ${{PULP_WORKER_MEMORY_LIMIT}}
    ingress_type: nodeport
    pulp_settings:
      domain_enabled: ${{DOMAIN_ENABLED}}
      redis_db: 1
    database:
      external_db_secret: ${{DB_SECRET_NAME}}
    cache:
      enabled: true
      external_cache_secret: content-sources-pulp-in-memory-db
    object_storage_s3_secret: content-sources-pulp-s3-for-operator
    image: ${{IMAGE}}
    image_version: ${{IMAGE_TAG}}
    image_web_version: latest
    inhibit_version_constraint: true
    telemetry:
      enabled: true
      resource_requirements:
        limits:
          cpu: ${{OTEL_COLLECTOR_CPU_LIMIT}}
          memory: ${{OTEL_COLLECTOR_MEMORY_LIMIT}}
        requests:
          cpu: ${{OTEL_COLLECTOR_CPU_REQUEST}}
          memory: ${{OTEL_COLLECTOR_MEMORY_REQUEST}}
parameters:
  - name: ENV_NAME
    description: Specify your (ephemeral) namespace
    required: true
  - name: IMAGE
    description: Specify which container image the operator will deploy as Pulp services.
    value: quay.io/cloudservices/pulp-rpm-ubi
  - name: IMAGE_TAG
    description: Specify the tag or hash for the image deployed by the operator.
    value: latest
  - name: DOMAIN_ENABLED
    description: Pulp setting that determines if domains are enabled.
    value: "false"
  - name: PULP_API_REPLICAS
    description: Number of pulp api replicas
    value: "1"
  - name: PULP_WEB_REPLICAS
    description: Number of pulp web replicas
    value: "0" # setting to 0 because we are using an ingress as a reverse proxy
  - name: PULP_WORKER_REPLICAS
    description: Number of pulp workers
    value: "1"
  - name: PULP_CONTENT_REPLICAS
    description: Number of pulp content replicas
    value: "1"
  - name: PULP_API_CPU_REQUEST
    description: Amount of CPU to request for the API pods
    value: "250m"
  - name: PULP_CONTENT_CPU_REQUEST
    description: Amount of CPU to request for the Content pods
    value: "250m"
  - name: PULP_WORKER_CPU_REQUEST
    description: Amount of CPU to request for the Worker pods
    value: "250m"
  - name: PULP_WEB_CPU_REQUEST
    description: Amount of CPU to request for the nginx pods
    value: "250m"
  - name: PULP_API_CPU_LIMIT
    description: Limit of CPU use by API pods
    value: "500m"
  - name: PULP_CONTENT_CPU_LIMIT
    description: Limit of CPU use by Content pods
    value: "500m"
  - name: PULP_WORKER_CPU_LIMIT
    description: Limit of CPU use by Worker pods
    value: "500m"
  - name: PULP_WEB_CPU_LIMIT
    description: Limit of CPU use by nginx pods
    value: "500m"
  - name: PULP_API_MEMORY_REQUEST
    description: Amount of memory to request for API pods
    value: "256Mi"
  - name: PULP_CONTENT_MEMORY_REQUEST
    description: Amount of memory to request for Content pods
    value: "256Mi"
  - name: PULP_WORKER_MEMORY_REQUEST
    description: Amount of memory to request for Worker pods
    value: "256Mi"
  - name: PULP_WEB_MEMORY_REQUEST
    description: Amount of memory to request for nginx pods
    value: "256Mi"
  - name: PULP_API_MEMORY_LIMIT
    description: Limit of memory use by API pods
    value: "512Mi"
  - name: PULP_CONTENT_MEMORY_LIMIT
    description: Limit of memory use by Content pods
    value: "512Mi"
  - name: PULP_WORKER_MEMORY_LIMIT
    description: Limit of memory use by Worker pods
    value: "512Mi"
  - name: PULP_WEB_MEMORY_LIMIT
    description: Limit of memory use by nginx pods
    value: "512Mi"
  - name: OTEL_COLLECTOR_CPU_REQUEST
    description: Amount of CPU to request for OpenTelemetry Collector container
    value: "250m"
  - name: OTEL_COLLECTOR_CPU_LIMIT
    description: Limit of CPU use by OpenTelemetry Collector container
    value: "500m"
  - name: OTEL_COLLECTOR_MEMORY_REQUEST
    description: Amount of memory to request for OpenTelemetry Collector container
    value: "256Mi"
  - name: OTEL_COLLECTOR_MEMORY_LIMIT
    description: Limit of memory use by OpenTelemetry Collector contaienr
    value: "512Mi"
  - name: DB_SECRET_NAME
    description: Name of the secret with external database information for the operator.
    value: "content-sources-pulp-external-database"
